{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and numpy reference\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're coming from DSCI 101 or 102, you've been working with very specific data objects (the `datascience` **table**). We've been using these tables for teaching purposes, but the real world of Python for data science uses the **DataFrame**, or **df**, from the `pandas` package. They are, for all intents and purposes, identical to our tables. However, we can apply a wider array of functions and methods to dfs, and we work with them in slightly different ways. Methods we used in DSCI 101 and 102 won't work on dfs. We have made this module to introduce you to this new object and transition your workflow from the `datascience.table` to the `pandas.DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# You're familiar with numpy, but now we need to import pandas as well\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# You may need to change the path to connect to the data you want to use ...\n",
    "path = 'https://github.com/oregon-data-science/DSCI101/raw/main/data/'\n",
    "\n",
    "'imports complete'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also start using `matplotlib` for plotting instead of the `datascience` .plot and .hist methods. Again, this is a widely used framework in the real world of data science. We have a dedicated module for plotting in `matplotlib` separate from this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load matplotlib \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply a generic style to all plots\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1) [Using Numpy](#Section1)\n",
    "\n",
    "2) [Creating and Displaying DataFrames](#Section2)\n",
    "\n",
    "3) [Accessing Values and Subsetting DataFrames](#Section3)\n",
    "\n",
    "4) [Modifying DataFrames](#Section4)\n",
    "\n",
    "5) [Train and Test Splits](#Section5)\n",
    "\n",
    "6) [Quick Reference](#Section6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using numpy\n",
    "<a id='Section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an array, we use `np.array`. Note that this is different from the `make_array` you used in the `datascience` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = np.array([1,2,6,7])\n",
    "my_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often need to instatiate empty arrays that we wish to fill out iteratively with for-loops. You could do this in `datascience` with `make_array`, but in numpy we use `np.empty(0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = np.empty(0)\n",
    "my_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It has a length of 0\n",
    "len(my_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `np.append` to iteratively build an array by looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(10):\n",
    "    my_array = np.append(my_array, i)\n",
    "    \n",
    "my_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional indices\n",
    "\n",
    "To access values in numpy arrays, we use brackets `[]` to subset elements by their positional index. The equivalent process in `datascience` was the `.item()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array[2:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function used here is the same as in `datascience`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(my_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The percentile function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentile function is almost identical in numpy, only that the order in which the arguments are taken is switched: **the first argument should be your array, and the second your percentile**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(my_array, 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Creating, displaying and summarizing DataFrames\n",
    "<a id='Section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and loading DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we have general data structures called dictionaries. They are objects that have a 'key' (equivalent to a column name is a table) and values associated with each key (equivalent to data in a table column). Let's create a simple dictionary and then show how it can be turned into a 'pandas' DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "data_dict = {\n",
    "    'col_1': [3, 2, 1, 0],\n",
    "    'col_2': ['a', 'b', 'c', 'd']\n",
    "}\n",
    "\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though a dictionary doesn't look like a table, it can act like one. This type of notation is the basis for most forms of data objects, table or otherwise. Accordingly, we can easily convert dictionaries to a DataFrame with `pd.DataFrame()` or `from_dict()`. \n",
    "\n",
    "Note that the dictionary **needs to be rectangular** for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny test DataFrame\n",
    "data_df = pd.DataFrame(data_dict)\n",
    "\n",
    "data_df.head()  # The bold numbers on the left are row index values, dictionary keys become table column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a <font color='blue'>.csv file</font> into a pandas <font color='blue'>DataFrame</font>\n",
    "\n",
    "While it's good to know how to create DataFrames manually and there are occasional uses for this skill, you will most commonly load data with Python from external sources. This is functionally identical to `read_table()`. As a reminder, .csv stands for **c**omma **s**eparated **v**alues, and is a very common format for storing tabular data as a file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>pd.read_csv(</font><font color='gray'>path_to_file</font><font color='blue'>)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a DataFrame (similar to a table) from a .csv file\n",
    "sky_df = pd.read_csv(path + 'skyscrapers_v2.csv')\n",
    "\n",
    "# Display the first 9 rows of data along with column names\n",
    "sky_df.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `.head()` is similar to `.show()` from the datascience package. Other tools for getting a quick glance at your data include `.tail()` (shows the last rows of data) and `.sample()` (you know what sample does). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying your DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most datasets we work with will be far too large to look at the entire thing. The following methods will let us look at small parts of the DataFrame to develop a sense of the shape, structure and properties of our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='gray'>df</font><font color='blue'>.head(</font><font color='gray'></font><font color='blue'>)</font> and <font color='gray'>df</font><font color='blue'>.tail(</font><font color='gray'></font><font color='blue'>)</font> Display beginning or end of your DataFrame\n",
    "`\n",
    "df.head(5) # Show 1st 5 rows\n",
    "df.tail(5)  # Show last 5 rows\n",
    "df.sample(n=3) # Show random 3 rows\n",
    "df.sample(frac=0.15 # Show random 0.15 (15%) of rows\n",
    "df.sample(n=4, random_state=311) # show same random 4 rows each time\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the first 5 rows of your data\n",
    "sky_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 5 rows of your data\n",
    "sky_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='gray'>df</font><font color='blue'>.sample(</font><font color='gray'></font><font color='blue'>)</font> Retrieve random rows of your DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method you should be familiar with. We can sample by number of rows (**n**) or by fraction of rows (**frac**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 random rows of your data\n",
    "sky_df.sample(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly display 10% of your data\n",
    "sky_df.sample(frac = 0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note than when we display subsets of our data, the total number of displayed rows is capped at 10 by default. If we displayed all 178 rows from our 10% sample, the resulting table would take up the majority of the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display info about columns in your DataFrame\n",
    "\n",
    "- `df.shape` # Returns dimensions of DataFrame\n",
    "- `df.info()` # info about each column  \n",
    "- `df.describe()` # statistics about each column of numbers  \n",
    "- `df['column_name'].unique()` # list of all unique values found in a column  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools we just introduced are all great for quick checks on our data. Did the DataFrame import as I expected? Do I have the right number of columns and the right type of data in each?\n",
    "\n",
    "We'll now show you a few ways to explore our DataFrames in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>pd.shape</font> # The dimensions of our DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.shape` returns the number of rows and columns in our data, telling us how large our dataset is (rows * columns\"), how many variables we have (columns), and how many observations we have (rows). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sky_df.shape   # (number of rows, number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = sky_df.shape[0]\n",
    "num_cols = sky_df.shape[1]\n",
    "\n",
    "print(f'{num_rows} rows, {num_cols} columns in sky_df data frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note another trick we just employed. The bracket subset in the form `object_name[index]` lets us pull out values from an array based on their location (index) in the array. `sky_df.shape[0]` means take the first value from the returned shape method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>df.info( )</font> # Type of each column, how many non-missing values in each column, memory use\n",
    "\n",
    "`.info()` returns a lot of useful information about your DataFrame. This includes the class of object (DataFrame in this case), the number of rows, the columns and their names, the number of observations in each column without missing values (NULL or NA), and the type of data in each column.\n",
    "\n",
    "It also tells you much of your computer's working memory (RAM) is occupied by the DataFrame. This is not an issue we've run into in DSCI101 and 102, but could be a problem when working with datasets w/ hundreds of thousands or millions of observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='blue'>df.describe( )</font> # count, mean, std, min, max of each numeric column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about some quick statistics on all of your numeric variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sky_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>df['col-A'].unique( )</font> Unique values in a column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can easily look at all unique values in a column with `.unique()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_df['material'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Accessing values and subsetting DataFrames\n",
    "<a id='Section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's one thing to *look* at your values, but *doing* anything with them requires tools for accessing only those values we want. Below we'll show how to access specific values by\n",
    "\n",
    "    1) Selecting rows/columns by their positional index\n",
    "    \n",
    "    2) Selecting columns by their name\n",
    "    \n",
    "    3) Subsetting rows with logical expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsets with indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>take some rows</font>  using row numbers (0 is first row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously demonstrated the use of brackets to subset arrays. We'll do something similar again to subset DataFrames in two dimensions (rows and columns) using the `.iloc[]` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the value of the 5th row in the 1st column \n",
    "sky_one_index = sky_df.iloc[4,0]\n",
    "sky_one_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also subset using multiple indices at once. An easy way to do so is with the `:` operator. It functions by returning all integers from a number on the left to the number on the right. When we include only `:` as our index, this simply means *all* values possible in the data object.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return rows 3, 4 and 5, and return all columns\n",
    "sky_via_index_numbers = sky_df.iloc[3:6, :] # Rows 3, 4, 5 but not 6, all columns :\n",
    "sky_via_index_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsets by column name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>which columns</font> to use in a new data frame</font> --like <font color='gray'>datascience</font> <font color='darkgreen'>.select( )</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've already discovered, it's often easier or necessary to subset out tables for just the variables we need. Conveniently, we can do this again with bracket notation. Instead of passing numeric indices, we can pass an array of column names to take. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_use = ['completed', 'city', 'height']\n",
    "sky2_df = sky_df[columns_to_use]\n",
    "sky2_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='brown'>[[ ... ]]</font> is <font color='gray'>more traditional in Pandas</font> # but also <font color='darkred'>more error-prone</font>\n",
    "\n",
    "Forgetting to use double [[ ]] is a very common error;\n",
    "\n",
    "Use a list and then it's just my_df[my_list_of_columns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky2_df = sky_df[['completed', 'city', 'height']]  # Without [[ ]] it won't work right!\n",
    "sky2_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsets with logical expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Get selected rows</font>  from pandas data frame  -like <font color='gray'>datascience</font> <font color='darkgreen'>.where( )</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index and column subsets have their uses, but the ability to subset our data based on specific conditions is workhorse tool in the data science toolbox. In 101 and 102 we used `.where()` to do this, but going forward we'll use the **logical subset**. \n",
    "\n",
    "1) Evaluate values in a column against against conditions using logical operators, attaining a new column of True/False values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logical operators** and their equivalents in the `datascience` package. \n",
    "\n",
    "- `==` Is equal to; `are.equal_to()`                 \n",
    "- `!=` Not equal to; `are.not_equal_to()`    \n",
    "- `>`  Greater than; `are.above()`           \n",
    "- `>=` Greater than or equal to; `are.above_or_equal_to()`   \n",
    "- `<`  Less than; `are.below()`                \n",
    "- `<=` Less than or equal to; `are.below_or_equal_to()`\n",
    "- `in` Contained in; `are.containg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a True / False column, filtering by year completed ...\n",
    "np.random.seed(1111) # to be able to repeat and get same results each time\n",
    "\n",
    "# Evaluate every value in the 'completed' column against the logical expression 'is equal to 1979'\n",
    "select_1979 = sky_df['completed'] == 1979\n",
    "\n",
    "select_1979.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Using these True/False values, bracket subset our data returning only those rows associated with True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_1979 = sky_df['completed'] == 1979\n",
    "\n",
    "# Only put True rows into sky_1979_df\n",
    "sky_1979_df = sky_df[select_1979]  \n",
    "\n",
    "sky_1979_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, skip the middle-object and do the whole subset in one go, nesting our logical expression in our subset brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may see pandas code to get all buildings completed in 1979 using this style\n",
    "sky_1979_bldgs = sky_df[sky_df['completed'] == 1979]\n",
    "sky_1979_bldgs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all buildings completed <font color='blue'>before 1979</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_before_1979 = sky_df['completed'] < 1979\n",
    "\n",
    "# Only put True rows into sky_before_1979_df\n",
    "sky_before_1979_df = sky_df[select_before_1979]  \n",
    "\n",
    "sky_before_1979_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the items that are in a list using <font color='blue'>.isin(list of values)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows associated with the years 1899, 1904, 1921\n",
    "select_dates = sky_df['completed'].isin([1899, 1904, 1921])\n",
    "\n",
    "# select dates is an array of True / False values\n",
    "sky_selected_years = sky_df[select_dates]\n",
    "\n",
    "sky_selected_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows associated with the cities Atlanta, Sacramento, San Diego\n",
    "select_cities = sky_df['city'].isin(['Atlanta', 'Sacramento', 'San Diego'])\n",
    "\n",
    "# select_cities is now an array of True / False values\n",
    "\n",
    "sky_selected_cities = sky_df[select_cities]\n",
    "sky_selected_cities.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show buildings (built in 1924)</font><font color='red'> & </font> (taller than 150 meters)<font color='red'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're not limited by singular logical expressions. We can use multiple expressions to string together filters on our data. The most common operators are **and** represented by `&` in 'pandas', and **or**, represented by `|`. When subsetting with multiple conditions, `&` means that both logical expressions must be true for a row to be returned, while `|` means that either logical expression can be true for a row to be returned. \n",
    "\n",
    "Note: \"and\" would operate on an entire column but pandas says \"this does not make sense\"   \n",
    "Note2: & (bitwise \"and\") used in Pandas, works element by element   \n",
    "Note3: Need parentheses ( ) & ( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ( ) parentheses needed \n",
    "select_1924_and_tall = (sky_df['completed'] == 1924) & (sky_df['height'] > 150.0)\n",
    "#                      *                           *   *                        *\n",
    "sky_old_tall = sky_df[select_1924_and_tall]\n",
    "sky_old_tall.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_1924 = sky_df['completed'] == 1924\n",
    "select_tall  = sky_df['height'] > 150.0\n",
    "\n",
    "sky_old_tall3 = sky_df[select_1924 & select_tall]\n",
    "\n",
    "sky_old_tall3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want either of two groups, we use `|` instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Either (completed in 1904 ) <font color='red'> | </font>  (city is Sacramento)\n",
    "\n",
    "`select_groups = (sky_df['city'] == 'Sacramento') | (sky_df['completed'] == 1904) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, remember the ( )\n",
    "select_city_or_date = (sky_df['city'] == 'Sacramento') | (sky_df['completed'] == 1904) \n",
    "#                     *                              *   *                           *    \n",
    "sky_city_or_date = sky_df[select_city_or_date]\n",
    "sky_city_or_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset with 'query'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good time to remind you about best practices in variable names, including column names. We should always avoid spaces in variable names (`year_completed` instead of `year completed`), as well as special characters (`tax_percent` instead of `tax%`). This is particularly important for `.query()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use df<font color='blue'>.query( )</font> to select rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.query()` lets us select rows using expressions like above, but with different notation. Whereas above we created arrays of True/False and used these to subset in bracket notation, `.query()` is a method that subsets for us given a logical expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all buildings completed in 1980\n",
    "sky_1980 = sky_df.query('completed == 1980')\n",
    "sky_1980.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get buildings completed before 1969\n",
    "sky_pre_1969 = sky_df.query('completed < 1969')\n",
    "sky_pre_1969.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get buildings completed in any of theses years [1899, 1904, 1921]\n",
    "sky_years = sky_df.query('completed in [1899, 1904, 1921]')\n",
    "sky_years.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show buildings (built in 1924) and (taller than 150 meters)\n",
    "sky_1924_or_tall = sky_df.query('(completed == 1924) and (height > 150)')\n",
    "sky_1924_or_tall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City Portland or height >= 400\n",
    "sky_weird_or_tall = sky_df.query('(city == \"Portland\") or  (height >= 400)')\n",
    "sky_weird_or_tall.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Modifying DataFrames\n",
    "<a id='Section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll wrap up with a buffet of options for modifying our DataFrames, including sorting, changing values, and adding new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sorting by columns, we can reorganize our DataFrame in logical orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>.sort_values</font>(<font color='gray'>'city'</font>) # sort by data frame column</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sort by a column of string values, the DataFrame is reordered in alphabetical order of that column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_by_city = sky_df.sort_values('height')\n",
    "\n",
    "sky_by_city.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>.sort_values</font>(<font color='gray'>'completed'</font><font color='red'>, ascending=False</font>) # biggest numbers first</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can sort by numeric order of a column. The default way to do this is from lowest values to highest, but we can toggle the `ascending` argument to False to sort from highest values to lowest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_by_years = sky_df.sort_values('completed', ascending=False) # big numbers first\n",
    "sky_by_years.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>sort_values</font>(<font color='red'>['city', 'completed']</font>) # by data frame column list</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting DataFrames is not limited to a single column. If we pass an array of columns to `.sort_values()`, we can sort each column in the order they're listed. In this case, the second column will be sorted nested within the first column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_by_city_year = sky_df.sort_values(['city', 'completed'])\n",
    "sky_by_city_year.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>sort_values(['city', 'completed'], </font><font color='red'>ascending=</font>[True, False]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also dictate the direction of our sort for each column passed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_by_city = sky_df.sort_values(['city', 'completed'], ascending=[True, False])\n",
    "sky_by_city.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DataFrame columns as 'numpy' arrays\n",
    "\n",
    "Many vectorized operations (operations that apply iteratively to every element in an array) work specifically on 'numpy' arrays. This means its often important to extract columns in a DataFrame as an array object. This involves two steps: \n",
    "\n",
    "1) Identify the column   \n",
    "\n",
    "2) Convert that column to numpy   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my_array = df['column_name']<font color='blue'>.to_numpy()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array from pandas data frame column\n",
    "year_completed = sky_df['completed'].to_numpy()\n",
    "year_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively\n",
    "np.array(sky_df['completed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a <font color='blue'>new age column</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ability to perform vectorized operations on 'numpy' arrays, we can easily create a new column giving us the age of each building. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sky_df['age'] = 2022 - sky_df['completed']\n",
    "sky_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and reshaping data\n",
    "<a id='Section4_2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>.groupby('column_name')</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlate to 'datascience' `.group()` is `.groupby()` in 'pandas. The application is basically the same: choose a column to group by, usually some sort of categorical variable or factor, and apply a method to each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group by city and count the number of rows/observations associated with each\n",
    "sky_by_city = sky_df.groupby('city')['city'].count()\n",
    "sky_by_city.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group by city and find the max height (the tallest building) in each\n",
    "sky_by_city = sky_df.groupby('city')['height'].max()\n",
    "sky_by_city.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by city and compute the average age of tall buildings\n",
    "sky_by_city_average = sky_df.groupby('city')['age'].mean()\n",
    "sky_by_city_average.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not limited to a single grouping factor - we can split up our data along any combination of variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by both city and material, then find the maximum height of each combination\n",
    "sky_by_city_material = sky_df.groupby(['city', 'material'])['height'].max()\n",
    "sky_by_city_material.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>.pivot_table()</font> # A core function in reshaping data, often better than groupby-apply sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is a long one, so we'll refer you to this guided tutorial:\n",
    "\n",
    "The full URL\n",
    "https://pbpython.com/pandas-pivot-table-explained.html \n",
    "\n",
    "Well worth checking it out. Starts with the basics, then adds a feature at a time to achieve some minor goal.\n",
    "\n",
    "The excel file the examples use is in our github DSCI101 area so once you define path (in the usual way), you will be able to follow along with the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this \n",
    "# df = pd.read_excel(\"../in/sales-funnel.xlsx\")\n",
    "# df.head()\n",
    "\n",
    "# To this -- then run the code shown in the Practical Business Python site (see URL above...)\n",
    "df = pd.read_excel(path + 'sales-funnel.xlsx')\n",
    "df.head()\n",
    "\n",
    "# Enjoy the article, and enjoy the step by step approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge two DataFrames -- like datascience *join*\n",
    "<a id='Section4_3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.merge()` in 'pandas' (or `.join()` as you know it) takes two separate DataFrames and merges them into one using a **key**. A key is a common identifying variable shared between two datasets. Note that the presence of this key makes these datasets **relational**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read nba player data from 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = pd.read_csv(path + 'nba2013.csv')\n",
    "nba.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read nba salary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_salary = pd.read_csv(path + 'nba_salaries.csv')\n",
    "nba_salary.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>merge left and right </font> data frames </font> -like <font color='gray'>datascience</font> <font color='darkgreen'>.join( )</font> \n",
    "\n",
    "When we talk about 'left' and 'right' DataFrames, we're typically talking about the original DataFrame to which we're adding new information (the left) and the DataFrame that is the source of this new information (the right). This is not always the case, merges can go in the other direction and sometimes we value information in both DataFrames equally, but is a useful convention. \n",
    "\n",
    "Let's demo two common merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left merge is a merge that preserves all information in the left DataFrame, and incorporates only information from the right DataFrame with a matching key. Below we do a left merge using player name as our key (specifying the column name for our key in left and right DataFrames respectively). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_left = pd.merge(nba, nba_salary,                     \n",
    "                   how='left',                         \n",
    "                   left_on='Name',right_on='PLAYER')    \n",
    "nba_left.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with added information where Name (left) and PLAYER (right) are the same. Because we did a left merge, we preserve the entirety of the left DataFrame and therefore end up with null values where there was no match on our key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inner merge is also common, only keeping rows from both the left and right DataFrames where their key matches. Information from both DataFrames can be lost with this merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nba_all = pd.merge(nba, nba_salary,                     \n",
    "                   how='inner',                         \n",
    "                   left_on='Name',right_on='PLAYER')    \n",
    "nba_all.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options include: \n",
    "- 'outer' merge where all information is kept from both DataFrames, resulting in null values added to both left and right DataFrames. \n",
    "- 'right' merge, where all information is preserved in the right DataFrame but not the left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing values from your DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing null values can be important when calculating statistics or feeding data to models. For example, many functions that calculate averages will throw errors if fed missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>.dropna( )</font> drops all rows with any NaN value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_sal_plus = nba_all.dropna()\n",
    "\n",
    "nba_sal_plus.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace values in your DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily change values in our DataFrames with `.replace()`. This method is often invoked in data cleaning or feature engineering when values need tweaked, re-encoded, or corrected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "family = pd.read_csv(path + 'family_heights.csv')\n",
    "family.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace some values in a column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change values in a column, we pass a dictionary to our method `.replace()` with `key:value` pairs corresponding to our old values on the left and new values on the right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in a column\n",
    "family['sex'] = family['gender'].replace({'male': 0, 'female': 1}) # {old:new, old2:new2}\n",
    "family.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>'One hot encoding'</font> - changes category column into <font color='blue'>several 0/1 (yes/no) columns</font>, one column per unique category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables (factors) can be both **ordinal**, meaning they have quantitative meaning *e.g. tax bracket*, and **nominal**, meaning they have no natural order or quantitative meaning *e.g. eye color*. **One hot encoding** is the process of turning a single nominal categorical variable into multiple binary (1/0) variables. This format is often more tractable in statistical models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column to \"One Hot Encoding\" using get_dummies\n",
    "\n",
    "# In general, with n unique values, with get_dummies you will get n columns, all 0 except for rows\n",
    "# that matche the column's category\n",
    "#\n",
    "sky_new = pd.get_dummies(sky_df, columns=['material'])\n",
    "sky_new.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'material' column is now replaced with as many columns as unique values of material. Each value is either a 0, meaning not that material, or a 1, meaning yes that material. This process side-steps arbitrary ordering of our material variable - the factor levels initially corresponding to each variable were meaningless (*e.g.* concrete == 1, steel == 2, mix == 3) but still implied ordination. No such ordination is implied after one hot encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we rename values, we can also rename columns by using a dictionary with the `.rename()` method. \n",
    "\n",
    "Handy when you need to use `df.query()`. Query must have columns with names that work as python variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sky_df = sky_df.rename(columns={'name':'building', 'completed': 'year_built', })\n",
    "my_sky_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Split data into Train and Test subsets\n",
    "<a id='Section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often need to split our data into subsets, one for training some model and the other for testing the quality of our model. The following is a demonstration of one way to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv(path + 'wine.csv')\n",
    "wine.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To split into 89, 89 Train, Test, start with shuffle of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state= some number lets you rerun and get the same random data\n",
    "#               not required, but handy for testing\n",
    "shuffle_wine = wine.sample(frac=1.0, random_state=311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_train = shuffle_wine[0:89] # 1st 89 rows\n",
    "wine_test = shuffle_wine[89:]   # remaining rows\n",
    "wine_train.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_Class = wine_train['Class']\n",
    "train_Class.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_attributes = wine_train.drop(columns=['Class'])\n",
    "train_attributes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Actual_Class = wine_test['Class']\n",
    "test_Actual_Class.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attributes = wine_test.drop(columns=['Class'])\n",
    "test_attributes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick reference \n",
    "<a id='Section6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas handles tables of data; it calls a table a <font color='blue'>__DataFrame__</font> or df\n",
    "\n",
    "```python \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read a csv file\n",
    "my_df = pd.read_csv(path + 'my_file.csv') # instead of Table.read_table\n",
    "\n",
    "my_df.head(6) # show 1st 6 rows of my_df data frame\n",
    "my_df.tail(5) # show last 5 rows of my_df data frame\n",
    "\n",
    "my_df.shape      # Shows number of rows, number of columns in your data frame\n",
    "my_df.info()     # Shows each column name, number of non-empty items, type of column (int, float, object)\n",
    "my_df.describe() # Shows count, mean, std, min, 25th percentile, 50th, 75th, max for each numeric column\n",
    "\n",
    "# Rename some columns\n",
    "my_df.rename(columns={'old col 1':'new_col1', 'another old': 'another_new', })\n",
    "\n",
    "# variations on display of DataFrame rows\n",
    "sky_sample_1_percent = sky_df.sample(frac=0.01) # random sample of 1% of rows in the data frame\n",
    "sky_sample_6 = sky_df.sample(n=6) # random sample of 6 rows in the data frame\n",
    "sky_sample_7 = sky_df.sample(n=7, random_state=311)   # random sample of 7, repeatable results\n",
    "\n",
    "# .iloc[]  -- use index numbers for rows, columns to work with\n",
    "sky_by_index = sky_df.iloc[3:6, :] # Rows 3, 4, 5 not 6, all columns -- like datascience .take()\n",
    "\n",
    "my_df['a column'].unique() # Shows list of the unique value in this column\n",
    "\n",
    "columns_list = ['name', 'age', 'major'] # Put columns in the order you want in your df\n",
    "my_subset = my_df[columns_list] # Gets df with just those 3 columns  (like .select)\n",
    "\n",
    "# sort_values\n",
    "my_sorted_df = my_df.sort_values('major') # Sort by major, A to Z order\n",
    "my_gpa_sort = sort_values('gpa', ascending=False) # biggest gpa numbers first\n",
    "\n",
    "my_df2 = sort_values(['major', 'name']) # by major, within major by name\n",
    "my_df3 = sort_values(['major', 'gpa'], ascending=[True, False]) # by major then by biggest gpa firs\n",
    "\n",
    "# Convert df column to a numpy array  -- like datascience .column()\n",
    "my_array = df['gpa'].to_numpy()\n",
    "\n",
    "# Get selected rows from pandas data frame like datascience's .where( )\n",
    "\n",
    "select_1979 = sky_df['completed'] == 1979\n",
    "sky_1979_df = sky_df[select_1979] # Only put True rows into sky_1979_df \n",
    "\n",
    "select_before_1979 = sky_df['completed'] < 1979 \n",
    "sky_before_1979_df = sky_df[select_before_1979] # Only put True rows into sky_before_1979_df \n",
    "\n",
    "# Get all the items that are in a list using .isin(list of values)\n",
    "select_dates = sky_df['completed'].isin([1899, 1904, 1921]) # select dates is array of True / False values\n",
    "sky_selected_years = sky_df[select_dates]\n",
    "\n",
    "# Show buildings (built in 1924) & (taller than 150 meters) # Needs ( ) & ( )\n",
    "select_1924_and_tall = (sky_df['completed'] == 1924) & (sky_df['height'] > 150.0)\n",
    "sky_old_tall = sky_df[select_1924_and_tall]\n",
    "\n",
    "# sky_old_tall = sky_df[select_1924 & select_tall] # another way\n",
    "select_1924 = sky_df['completed'] == 1924\n",
    "select_tall  = sky_df['height'] > 150.0\n",
    "sky_old_tall3 = sky_df[select_1924 & select_tall]\n",
    "\n",
    "# list of items from 1 column\n",
    "select_cities = sky_df['city'].isin(['Atlanta', 'Sacramento', 'San Diego']) # array of True / False\n",
    "sky_selected_cities = sky_df[select_cities]\n",
    "\n",
    "# Either (completed in 1904 ) | (city is Sacramento)  # | means 'or'\n",
    "select_city_or_date = (sky_df['city'] == 'Sacramento') | (sky_df['completed'] == 1904) \n",
    "sky_city_or_date = sky_df[select_city_or_date]\n",
    "\n",
    "# df.query('city == \"Chicago\"') # works only when column names will be OK python variable names\n",
    "sky_1980 = sky_df.query('completed == 1980')\n",
    "\n",
    "# Create a new column\n",
    "sky_df['age'] = 2021 - sky_df['completed'] # Stores result in named column on left of =\n",
    "\n",
    "# Group by city, get counts\n",
    "sky_by_city = sky_df.groupby('city')['city'].count()\n",
    "\n",
    "# Group by city then within each city group, apply the max function to just the height column\n",
    "sky_by_city = sky_df.groupby('city')['height'].max()\n",
    "\n",
    "# Group by city and material, find tallest within each group\n",
    "sky_by_city_material = sky_copy.groupby(['city', 'material'])['height'].max()\n",
    "\n",
    "# pivot_table # The full URL https://pbpython.com/pandas-pivot-table-explained.html\n",
    "df.pivot_table(index=[\"Manager\",\"Rep\"])\n",
    "\n",
    "# merge -- like datascience join\n",
    "\n",
    "# 3 varieties\n",
    "# 1) inner merge (result must appear in both left and right data frames\n",
    "nba_all = pd.merge(nba, nba_salary,                     # left is nba, right is nba_salary\n",
    "                   how='inner',                         # must get data from both left and right data frames\n",
    "                   left_on='Name',right_on='PLAYER')    # join on Name from left, PLAYER from right\n",
    "\n",
    "# 2) left merge (result keeps every item in left data frame, matches with right when possible\n",
    "#    Non-matches show as NaN (\"Not a Number\" -- missing value)\n",
    "nba_players = pd.merge(nba, nba_salary,                 # left is nba, right is nba_salary\n",
    "                   how='left',                          # gets all left data rows\n",
    "                   left_on='Name',right_on='PLAYER')    # join on Name from left, PLAYER from right\n",
    "\n",
    "# 3) right merge (result keeps every item in right data frame, matches with left when possible\n",
    "#    Non-matches show as NaN (\"Not a Number\" -- missing value)\n",
    "nba_salaries_plus = pd.merge(nba, nba_salary,            # left is nba, right is nba_salary\n",
    "                    how='right',                         # gets all right data rows\n",
    "                    left_on='Name',right_on='PLAYER')    # join on Name from left, PLAYER from right\n",
    "\n",
    "# .dropna( )\n",
    "nba_sal_plus = nba_salaries_plus.dropna( )  # .dropna() drops all rows with any NaN value\n",
    "\n",
    "# Change values in your DataFrame\n",
    "# Replace values in a column\n",
    "family['gender'] = family['gender'].replace({'male': 0, 'female': 1}) # {old:new, old2:new2}\n",
    "\n",
    "# 'One hot encoding' - changes category column into a series of yes/no columns\n",
    "sky_new = pd.get_dummies(sky_df, columns=['material'])\n",
    "# Material concrete, steel, \n",
    "# Kills material column, creates material_concrete, material_steel, \n",
    "\n",
    "# Options when reading .csv file\n",
    "\n",
    "# keyword options with read_csv:\n",
    "#  sep='\\t' lets you read tab-separated data (or provide some other separator value)\n",
    "#  header=None lets you read data without column names\n",
    "#  names=['Who', 'What', 'Where'] provide your own column names\n",
    "#  skiprows=3  Skips the first 3 rows (when you have notes and copyright before the real data)\n",
    "\n",
    "# Shuffle data, then break into train, test data frames\n",
    "shuffle_wine = wine.sample(frac=1.0, random_state=311) # Shuffle all data before a split\n",
    "wine_train = shuffle_wine[0:89] # 1st 89 rows\n",
    "wine_test = shuffle_wine[89:]   # remaining rows\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
